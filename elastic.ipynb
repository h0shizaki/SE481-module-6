{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Search Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElasticIndexer:\n",
    "    def __init__(self):\n",
    "        self.crawled_folder = Path(os.path.abspath('')).parent / 'crawled/'\n",
    "        print(self.crawled_folder  / 'url_list.pickle')\n",
    "        with open(self.crawled_folder / 'url_list.pickle', 'rb') as f:\n",
    "            self.file_mapper = pickle.load(f) \n",
    "        self.es_client = Elasticsearch('https://localhost:9200', basic_auth=(\"elastic\", \"6E0GWL_MEddnKJWCnk*M\"),\n",
    "                    ca_certs=\"./http_ca.crt\")\n",
    "        # self.es_client.info()\n",
    "\n",
    "    def run_indexer(self):\n",
    "        self.es_client.options(ignore_status=400).indices.create(index='simple')\n",
    "        self.es_client.options(ignore_status=[400,404]).indices.delete(index='simple')\n",
    "        for file in os.listdir(self.crawled_folder):\n",
    "            if file.endswith(\".txt\"):\n",
    "                j = json.load(open(os.path.join(self.crawled_folder, file)))\n",
    "                j['id'] = j['url']\n",
    "                # print(j)\n",
    "                self.es_client.index(index='simple', document=j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\SoftwareEngineer\\Work\\ir\\crawled\\url_list.pickle\n",
      "Got 19 Hits :\n",
      "The title is Gifted School 2020 (https://go.camt.cmu.ac.th/index.php/th/2019-05-16-09-02-18/2019-05-16-09-05-06)\n",
      "The title is Graduate School, Chiang Mai University (https://www.grad.cmu.ac.th/index.php?lang=en)\n",
      "The title is Gift School 2023 (https://service.camt.cmu.ac.th/gifted)\n",
      "The title is การจัดการความรู้และนวัตกรรม ป.เอก (https://go.camt.cmu.ac.th/index.php/th/major/graduate/doctoral-km)\n",
      "The title is CMU-IPAS (https://www1.reg.cmu.ac.th/reg-ipas/main/index.php)\n",
      "The title is วิศวกรรมซอฟต์แวร์ ป.โท (https://go.camt.cmu.ac.th/index.php/th/major/graduate/graduate-se)\n",
      "The title is วิทยาลัยศิลปะ สื่อ และเทคโนโลยี (http://go.camt.cmu.ac.th)\n",
      "The title is การจัดการความรู้และนวัตกรรม ป.โท (https://go.camt.cmu.ac.th/index.php/th/major/graduate/graduate-km)\n",
      "The title is PRE COLLEGE 2020 (https://go.camt.cmu.ac.th/index.php/th/2019-05-16-09-02-18/2019-05-16-09-05-7)\n",
      "The title is Young Mobile Dev (https://go.camt.cmu.ac.th/index.php/th/2019-05-16-09-02-18/2019-07-06-10-28-13/2019-05-16-09-05-8)\n"
     ]
    }
   ],
   "source": [
    "s = ElasticIndexer()\n",
    "s.run_indexer()\n",
    "query = {\"match\": { \"text\": \"school\"}} \n",
    "results = s.es_client.search(index='simple', query=query)\n",
    "print(\"Got %d Hits :\" % results['hits']['total']['value'])\n",
    "for hit in results['hits']['hits']:\n",
    "    print(\"The title is {0} ({1})\".format(hit['_source']['title'], hit['_source']['url']))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticsearch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from flask import Flask , request\n",
    "import pandas as pd\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.es_client  = Elasticsearch('https://localhost:9200', basic_auth=(\"elastic\", \"6E0GWL_MEddnKJWCnk*M\"),\n",
    "                    ca_certs=\"./http_ca.crt\")\n",
    "\n",
    "@app.route('/search_es', methods=[\"GET\"])\n",
    "def search_es():\n",
    "    start = time.time()\n",
    "    res = {'status': 'success'}\n",
    "    argList = request.args.to_dict(flat=False)\n",
    "    query_term = argList['query'][0]\n",
    "    results = app.es_client.search(index='simple', source_excludes=['url_lists'], size=100, \n",
    "                                   query={\"match\": {\"text\": query_term}})\n",
    "    end = time.time()\n",
    "    total_hit = results['hits']['total']['value']\n",
    "    results_df = pd.DataFrame([[hit[\"_source\"]['title'], hit[\"_source\"]['url'], hit[\"_source\"]\n",
    "['text'][:100], hit[\"_score\"]] for hit in results['hits']['hits']], columns=['title', 'url', 'text', \n",
    "'score'])\n",
    "    \n",
    "    res['total_hit'] = total_hit\n",
    "    res['results'] = results_df.to_dict('records')\n",
    "    res['elapse'] = end-start\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from ManualIndexer import Indexer, preProcessor\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.es_client  = Elasticsearch('https://localhost:9200', basic_auth=(\"elastic\", \"6E0GWL_MEddnKJWCnk*M\"),\n",
    "                    ca_certs=\"./http_ca.crt\")\n",
    "app.indexer = Indexer()\n",
    "app.indexer.run_indexer()\n",
    "\n",
    "@app.route('/manual_index', methods=[\"GET\"])\n",
    "def manual_index():\n",
    "    \n",
    "    start = time.time()\n",
    "    res = {'status': 'success'}\n",
    "    argList = request.args.to_dict(flat=False)\n",
    "    query_term = argList['query'][0]\n",
    "    results = app.indexer.search(query_term)\n",
    "    end = time.time()\n",
    "    total_hit = len(results)\n",
    "    \n",
    "    res['total_hit'] = total_hit\n",
    "    res['results'] = results.sort_values(\"score\", ascending=False).drop(\"url_lists\", axis=1).head(100).to_dict('records')\n",
    "    res['elapse'] = end-start\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [12/Feb/2024 18:19:51] \"GET /manual_index?query=school HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [12/Feb/2024 18:20:06] \"GET /manual_index?query=school HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run(debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
